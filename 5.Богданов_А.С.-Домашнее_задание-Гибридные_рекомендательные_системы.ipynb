{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "010dacc1-1a93-438f-b556-267d37e5e985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: surprise in /opt/conda/lib/python3.10/site-packages (0.1)\n",
      "Requirement already satisfied: scikit-surprise in /opt/conda/lib/python3.10/site-packages (from surprise) (1.1.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from scikit-surprise->surprise) (1.16.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.10/site-packages (from scikit-surprise->surprise) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.11.2 in /opt/conda/lib/python3.10/site-packages (from scikit-surprise->surprise) (1.22.4)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-surprise->surprise) (1.9.0)\n"
     ]
    }
   ],
   "source": [
    "    # установим недостающие библиотеки\n",
    "!pip install surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d860152b-5d04-4b44-b564-f0d0bcbcb140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# определим класс, который будет рекомендовать\n",
    "# на основе модели гибрида (блендинга)[]\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy\n",
    "from surprise import Reader\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "class MyRecommender:\n",
    "  default_models = ['random_forest','knn','svd']\n",
    "  default_weight = [0.3,0.3,0.4]\n",
    "  default_opts = {\n",
    "      'random_forest':{\n",
    "          'max_depth':8,\n",
    "          'max_features':3,\n",
    "          'n_estimators':500,\n",
    "      },\n",
    "      'knn':{\n",
    "          'n_neighbors':20,\n",
    "          'n_jobs':-1,\n",
    "          'metric':'manhattan',\n",
    "      },\n",
    "      'svd':{\n",
    "          'n_factors': 90,\n",
    "          'n_epochs': 100,\n",
    "          'lr_all': 0.01,\n",
    "          'reg_all': 0.1,\n",
    "          'biased': True,\n",
    "      },\n",
    "  }\n",
    "  default_meta_opts = {\n",
    "    'item_id_attr':'item_id',\n",
    "    'user_id_attr':'user_id',\n",
    "    'rating_attr':'rating',\n",
    "    'timestamp_attr':'timestamp',\n",
    "  }\n",
    "  models = {}\n",
    "  opts = {}\n",
    "  meta_opts = {}\n",
    "  weight = []\n",
    "  predictions = {}\n",
    "  X = None\n",
    "  Y = None\n",
    "\n",
    "  all_users = []\n",
    "  all_items = []\n",
    "\n",
    "  base_df = None\n",
    "\n",
    "  def __init__(self,models:list = [],model_weight:list = [],model_opts:dict = {},meta_opts:dict={}):\n",
    "    if(len(models) == 0):\n",
    "      models = self.default_models\n",
    "    if(len(model_weight) == 0):\n",
    "      weight = self.default_weight\n",
    "    if(len(models) != len(model_weight)):\n",
    "      raise Exception(\"Model count ({}) and weight count ({}) are differs\".format(len(models),len(weight)))\n",
    "\n",
    "    if(sum(model_weight) != 1.0):\n",
    "      raise Exception('Sum of weights must be 1. Current is {}'.format(sum(model_weight)))\n",
    "\n",
    "    for i,model in enumerate(models):\n",
    "      if model not in self.default_models:\n",
    "        raise Exception('Model name {} not supported'.format(model))\n",
    "      if model not in model_opts:\n",
    "        model_opts[model] = self.default_opts[model]\n",
    "      #if model not in meta_opts and model in self.default_meta_opts:\n",
    "      #  meta_opts[model] = self.default_meta_opts[model]  \n",
    "      self.models[model] = None\n",
    "      self.weight.append(model_weight[i])\n",
    "\n",
    "    for key in self.default_meta_opts:\n",
    "      if key not in meta_opts:\n",
    "        meta_opts[key] = self.default_meta_opts[key]\n",
    "\n",
    "    self.opts = model_opts\n",
    "    self.meta_opts = meta_opts\n",
    "\n",
    "  def prepare_random_forest(self,DATA):\n",
    "    X_columns = DATA.columns.tolist()\n",
    "    X_columns.remove(self.meta_opts['user_id_attr'])\n",
    "    X_columns.remove(self.meta_opts['item_id_attr'])\n",
    "    X_columns.remove(self.meta_opts['timestamp_attr'])\n",
    "    return DATA[X_columns]\n",
    "\n",
    "  def fit_random_forest(self,DATA):\n",
    "    X_cols = DATA.columns.tolist()\n",
    "    X_cols.remove(self.meta_opts['rating_attr'])\n",
    "    model = RandomForestRegressor(**self.opts['random_forest'])\n",
    "    model.fit(DATA[X_cols],DATA[self.meta_opts['rating_attr']])\n",
    "    return model\n",
    "\n",
    "  def predict_random_forest(self,user_id,num):\n",
    "    if(type(user_id) != list):\n",
    "        user_id = [user_id]\n",
    "      \n",
    "    u_predictions = {}\n",
    "    for uid in user_id:\n",
    "      item_df = pd.DataFrame(data=self.all_items,columns=[self.meta_opts['item_id_attr']])\n",
    "      user_df = self.DATA.query(self.meta_opts['user_id_attr']+' == @uid')[[self.meta_opts['item_id_attr'],self.meta_opts['rating_attr']]]\n",
    "      df = item_df.merge(user_df,on=self.meta_opts['item_id_attr'],how='left')\n",
    "\n",
    "      df_unwatched = df \\\n",
    "                     .query(self.meta_opts['rating_attr']+' != '+self.meta_opts['rating_attr'])\n",
    "      df_to_learn =  df_unwatched \\\n",
    "                     .merge(self.DATA.drop_duplicates(subset=[self.meta_opts['item_id_attr']]),on=self.meta_opts['item_id_attr'],how='left',suffixes=['','_r']) \\\n",
    "                     .drop([self.meta_opts['user_id_attr'],self.meta_opts['item_id_attr'],self.meta_opts['timestamp_attr'],self.meta_opts['rating_attr'],self.meta_opts['rating_attr']+'_r'],axis=1)\n",
    "      predictions = self.models['random_forest'].predict(df_to_learn)\n",
    "      df_unwatched['rating'] = predictions\n",
    "\n",
    "      df = df.merge(df_unwatched,on=self.meta_opts['item_id_attr'],how='left',suffixes=['','_r'])\n",
    " \n",
    "      df['score'] = df.loc[:,['rating','rating_r']].apply(lambda row: row[0] if row[1] != row[1] else row[1],axis=1)\n",
    "      df['rating_r'] = df['rating_r'].apply(lambda x: True if x != x else False)\n",
    "      df = df \\\n",
    "           .drop([self.meta_opts['rating_attr']],axis=1) \\\n",
    "           .rename(columns={'rating_r':'watched'}) \\\n",
    "           .query('watched == False') \\\n",
    "           .sort_values(by=['score'],ascending=False) \\\n",
    "           .head(num)\n",
    "      df['score'] = df['score'].apply(lambda x: 0 if x != x else x)\n",
    "      df['score'] = normalize([df['score'].to_numpy()])[0]\n",
    "      u_predictions[uid] = df\n",
    "    return u_predictions\n",
    "\n",
    "  def build_random_forest(self,DATA):\n",
    "    X_indexed = DATA.set_index([self.meta_opts['item_id_attr']])\n",
    "    df = self.get_user_item_df()\n",
    "    for idx,row in df.query('rating != rating').iterrows():\n",
    "      item_id = idx[1]\n",
    "      X_row = X_indexed.query(self.meta_opts['item_id_attr']+' == @item_id').iloc[0]\n",
    "      X_row = X_row.drop(self.meta_opts['user_id_attr'])\n",
    "      val = self.models['random_forest'].predict([X_row])\n",
    "      df.loc[idx]['rating'] = val[0]\n",
    "    \n",
    "  def prepare_knn(self,DATA,with_item_id=False):\n",
    "    X_columns = DATA.columns.tolist()\n",
    "    X_columns.remove(self.meta_opts['user_id_attr'])\n",
    "    if not with_item_id:\n",
    "      X_columns.remove(self.meta_opts['item_id_attr'])\n",
    "    X_columns.remove(self.meta_opts['rating_attr'])\n",
    "    X_columns.remove(self.meta_opts['timestamp_attr'])\n",
    "    return DATA.drop_duplicates(subset=[self.meta_opts['item_id_attr']])[X_columns]\n",
    "\n",
    "  def fit_knn(self,DATA):\n",
    "    model = NearestNeighbors(**self.opts['knn'])\n",
    "    model.fit(DATA)\n",
    "    return model\n",
    "\n",
    "  def predict_knn(self,user_id,num):\n",
    "    if(type(user_id) != list):\n",
    "        user_id = [user_id]\n",
    "      \n",
    "    u_predictions = {}\n",
    "    for uid in user_id:\n",
    "      item_df = pd.DataFrame(data=self.all_items,columns=[self.meta_opts['item_id_attr']])\n",
    "      user_df = self.DATA.query(self.meta_opts['user_id_attr']+' == @uid')[[self.meta_opts['item_id_attr'],self.meta_opts['rating_attr'],self.meta_opts['timestamp_attr']]]\n",
    "      df = item_df.merge(user_df,on=self.meta_opts['item_id_attr'],how='left')\n",
    "      df['watched'] = df[self.meta_opts['rating_attr']].apply(lambda x: False if x != x else True) \n",
    "        \n",
    "      df_user_top = df \\\n",
    "                     .query('watched == True') \\\n",
    "                     .sort_values(by=[self.meta_opts['rating_attr'],self.meta_opts['timestamp_attr']],ascending=False) \\\n",
    "                     .drop(['watched',self.meta_opts['timestamp_attr']],axis=1) \\\n",
    "                     .head(10)\n",
    "      df_to_learn =  df_user_top \\\n",
    "                     .merge(self.DATA.drop_duplicates(subset=[self.meta_opts['item_id_attr']]),on=self.meta_opts['item_id_attr'],how='left',suffixes=['','_r']) \\\n",
    "                     .drop([self.meta_opts['user_id_attr'],self.meta_opts['item_id_attr'],self.meta_opts['timestamp_attr'],self.meta_opts['rating_attr'],self.meta_opts['rating_attr']+'_r'],axis=1)\n",
    "      predictions = self.models['knn'].kneighbors(df_to_learn,return_distance=True)\n",
    "      df_pred = pd.DataFrame({\n",
    "          'indices':predictions[1].flatten(),\n",
    "          'distances':predictions[0].flatten(),\n",
    "      }).sort_values(by=['distances'],ascending=True)\n",
    "      indices = df_pred['indices'].to_numpy()\n",
    "      \n",
    "      df_recommend = self.prepare_knn(self.DATA,with_item_id=True).iloc[indices][[self.meta_opts['item_id_attr']]]\n",
    "      df_recommend['score'] = df_pred['distances']\n",
    "      df = df \\\n",
    "           .drop([self.meta_opts['rating_attr'],self.meta_opts['timestamp_attr']],axis=1) \\\n",
    "           .merge(df_recommend,on=self.meta_opts['item_id_attr'],how='left') \\\n",
    "           .query('watched == False') \\\n",
    "           .sort_values(by=['score'],ascending=True,na_position='last') \\\n",
    "           .head(num)\n",
    "      df['score'] = df['score'].apply(lambda x: 0 if x != x else x)\n",
    "      df['score'] = normalize([df['score'].apply(lambda x: df['score'].max() - x).to_numpy()])[0]\n",
    "      u_predictions[uid] = df\n",
    "    return u_predictions \n",
    "\n",
    "  def prepare_svd(self,DATA):\n",
    "    rating_min = DATA[self.meta_opts['rating_attr']].min()\n",
    "    rating_max = DATA[self.meta_opts['rating_attr']].max()\n",
    "    df = pd.DataFrame({\n",
    "        'uid':DATA[self.meta_opts['user_id_attr']].to_numpy().flatten(),\n",
    "        'iid':DATA[self.meta_opts['item_id_attr']].to_numpy().flatten(),\n",
    "        'rating':DATA[self.meta_opts['rating_attr']].to_numpy().flatten(),\n",
    "    })\n",
    "    reader = Reader(rating_scale=(rating_min, rating_max))\n",
    "    data = Dataset.load_from_df(df, reader)\n",
    "    return data.build_full_trainset()\n",
    "\n",
    "  def fit_svd(self,DATA):\n",
    "    model = SVD(**self.opts['svd'])\n",
    "    model.fit(DATA)\n",
    "    return model\n",
    "\n",
    "  def predict_svd(self,user_id,num):\n",
    "    if(type(user_id) != list):\n",
    "        user_id = [user_id]\n",
    "      \n",
    "    u_predictions = {}\n",
    "    testset = self.prepare_svd(self.DATA).build_anti_testset()\n",
    "    predictions = self.models['svd'].test(testset)\n",
    "    top_n = defaultdict(list)\n",
    "    \n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "      if uid in user_id:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    for uid, user_ratings in top_n.items():\n",
    "      user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "      top_n[uid] = user_ratings\n",
    "\n",
    "    for uid in user_id:\n",
    "      item_df = pd.DataFrame(data=self.all_items,columns=[self.meta_opts['item_id_attr']])\n",
    "      user_df = self.DATA.query(self.meta_opts['user_id_attr']+' == @uid')[[self.meta_opts['item_id_attr'],self.meta_opts['rating_attr'],self.meta_opts['timestamp_attr']]]\n",
    "      df = item_df.merge(user_df,on=self.meta_opts['item_id_attr'],how='left')\n",
    "      df['watched'] = df[self.meta_opts['rating_attr']].apply(lambda x: False if x != x else True) \n",
    "      \n",
    "      df_recommend = pd.DataFrame(data=top_n[uid],columns=[self.meta_opts['item_id_attr'],'score'])\n",
    "      df = df \\\n",
    "           .drop([self.meta_opts['rating_attr'],self.meta_opts['timestamp_attr']],axis=1) \\\n",
    "           .merge(df_recommend,on=self.meta_opts['item_id_attr'],how='left') \\\n",
    "           .query('watched == False') \\\n",
    "           .sort_values(by=['score'],ascending=False,na_position='last') \\\n",
    "           .head(num)\n",
    "      df['score'] = df['score'].apply(lambda x: 0 if x != x else x)\n",
    "      df['score'] = normalize([df['score'].to_numpy()])[0]\n",
    "      u_predictions[uid] = df\n",
    "    return u_predictions \n",
    "        \n",
    "\n",
    "  def fit(self,DATA):\n",
    "        \n",
    "    self.DATA = DATA\n",
    "    self.all_users = DATA[self.meta_opts['user_id_attr']].unique()\n",
    "    self.all_items = DATA[self.meta_opts['item_id_attr']].unique()\n",
    "    \n",
    "    for model in self.models:\n",
    "      self.fit_model(model)\n",
    "      #self.build_model(model,X,Y)\n",
    "      pass\n",
    "\n",
    "  def fit_model(self,model:str):\n",
    "    fit_method = 'fit_'+model\n",
    "    prepare_method = 'prepare_'+model\n",
    "    DATA_prepared = self.DATA\n",
    "    if hasattr(self,prepare_method):\n",
    "      DATA_prepared = getattr(self,prepare_method)(self.DATA)\n",
    "    self.models[model] = getattr(self,fit_method)(DATA_prepared)\n",
    "\n",
    "  def build_model(self,model:str):\n",
    "    build_method = 'build_'+model\n",
    "    #self.predictions[model] = getattr(self,build_method)(self.DATA)\n",
    "    \n",
    "  def get_user_item_df(self):\n",
    "    if self.base_df is not None:\n",
    "        return self.base_df\n",
    "\n",
    "    df_data = []\n",
    "    for user_id in self.all_users:\n",
    "      for item_id in self.all_items:\n",
    "        df_data.append([user_id,item_id,None])\n",
    "        \n",
    "    df = pd.DataFrame(data=df_data,columns=['user_id','item_id','rating'])\\\n",
    "              .set_index(['user_id','item_id'])\n",
    "    del df_data\n",
    "    \n",
    "    cur_data_df = pd.DataFrame({\n",
    "        'user_id':self.DATA[self.meta_opts['user_id_attr']],\n",
    "        'item_id':self.DATA[self.meta_opts['item_id_attr']],\n",
    "        'rating':self.DATA[self.meta_opts['rating_attr']],\n",
    "    }).set_index(['user_id','item_id'])\n",
    "\n",
    "    df = df.join(cur_data_df,on=['user_id','item_id'],rsuffix='_r') \\\n",
    "           .drop('rating',axis=1) \\\n",
    "           .rename(columns={'rating_r':'rating'})\n",
    "    self.base_df = df\n",
    "    return self.base_df\n",
    "        \n",
    "  def predict_model(self,model,user_id,num):\n",
    "    predict_method = 'predict_'+model\n",
    "    self.predictions[model] = getattr(self,predict_method)(user_id,num)\n",
    "\n",
    "  def weight_predictions(self,user_id,num):\n",
    "    r = {}\n",
    "    for i,model in enumerate(self.models):\n",
    "      for uid in self.predictions[model]:\n",
    "        if uid not in r:\n",
    "          r[uid] = {}\n",
    "        for idx,row in self.predictions[model][uid].iterrows():\n",
    "          if row[self.meta_opts['item_id_attr']] not in r[uid]:\n",
    "            r[uid][row[self.meta_opts['item_id_attr']]] = 0\n",
    "          r[uid][row[self.meta_opts['item_id_attr']]] += self.weight[i]*row['score']\n",
    "        r[uid] = dict(sorted(r[uid].items(), key=lambda item:item[1],reverse=True))\n",
    "    \n",
    "    lst = {}\n",
    "    for uid in r:\n",
    "      if uid not in lst:\n",
    "        lst[uid] = []\n",
    "      for item in r[uid]:\n",
    "        lst[uid].append(item)\n",
    "      lst[uid] = lst[uid][:num]\n",
    "\n",
    "    return lst\n",
    "    \n",
    "  def recommend(self,user_id, num:int = 10):\n",
    "    for model in self.models:\n",
    "        self.predict_model(model,user_id,num)\n",
    "    if(len(self.predictions) == 1):\n",
    "        return self.predictions[model]\n",
    "    return self.weight_predictions(user_id,num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbd043a2-6127-44ed-8f72-68aa03194aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# получим данные\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "links = pd.read_csv('https://raw.githubusercontent.com/ALKONDR/netology-recsys/master/lecture-1/links.csv')\n",
    "movies = pd.read_csv('https://github.com/ALKONDR/netology-recsys/raw/master/lecture-1/movies.csv')\n",
    "ratings = pd.read_csv('https://github.com/ALKONDR/netology-recsys/raw/master/lecture-1/ratings.csv')\n",
    "tags = pd.read_csv('https://github.com/ALKONDR/netology-recsys/raw/master/lecture-1/tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7536d89-54e1-46e0-b187-e4c59f41e177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tag</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>rating_mean</th>\n",
       "      <th>rating_cnt</th>\n",
       "      <th>rating_median</th>\n",
       "      <th>rating_variance</th>\n",
       "      <th>rating_mode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "      <td></td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>adventure|animation|children|comedy|fantasy</td>\n",
       "      <td>3.920930</td>\n",
       "      <td>215</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.693748</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "      <td></td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>comedy|romance</td>\n",
       "      <td>3.259615</td>\n",
       "      <td>52</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.091254</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "      <td></td>\n",
       "      <td>Heat (1995)</td>\n",
       "      <td>action|crime|thriller</td>\n",
       "      <td>3.946078</td>\n",
       "      <td>102</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.661308</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "      <td></td>\n",
       "      <td>Seven (a.k.a. Se7en) (1995)</td>\n",
       "      <td>mystery|thriller</td>\n",
       "      <td>3.975369</td>\n",
       "      <td>203</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.846684</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "      <td></td>\n",
       "      <td>Usual Suspects, The (1995)</td>\n",
       "      <td>crime|mystery|thriller</td>\n",
       "      <td>4.237745</td>\n",
       "      <td>204</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.638330</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100831</th>\n",
       "      <td>610</td>\n",
       "      <td>166534</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1493848402</td>\n",
       "      <td></td>\n",
       "      <td>Split (2017)</td>\n",
       "      <td>drama|horror|thriller</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.055556</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100832</th>\n",
       "      <td>610</td>\n",
       "      <td>168248</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493850091</td>\n",
       "      <td>heroic bloodshed</td>\n",
       "      <td>John Wick: Chapter Two (2017)</td>\n",
       "      <td>action|crime|thriller</td>\n",
       "      <td>4.142857</td>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.479592</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100833</th>\n",
       "      <td>610</td>\n",
       "      <td>168250</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1494273047</td>\n",
       "      <td></td>\n",
       "      <td>Get Out (2017)</td>\n",
       "      <td>horror</td>\n",
       "      <td>3.633333</td>\n",
       "      <td>15</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.882222</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100834</th>\n",
       "      <td>610</td>\n",
       "      <td>168252</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493846352</td>\n",
       "      <td></td>\n",
       "      <td>Logan (2017)</td>\n",
       "      <td>action|sci-fi</td>\n",
       "      <td>4.280000</td>\n",
       "      <td>25</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.401600</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100835</th>\n",
       "      <td>610</td>\n",
       "      <td>170875</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1493846415</td>\n",
       "      <td></td>\n",
       "      <td>The Fate of the Furious (2017)</td>\n",
       "      <td>action|crime|drama|thriller</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102677 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        userId  movieId  rating   timestamp               tag  \\\n",
       "0            1        1     4.0   964982703                     \n",
       "1            1        3     4.0   964981247                     \n",
       "2            1        6     4.0   964982224                     \n",
       "3            1       47     5.0   964983815                     \n",
       "4            1       50     5.0   964982931                     \n",
       "...        ...      ...     ...         ...               ...   \n",
       "100831     610   166534     4.0  1493848402                     \n",
       "100832     610   168248     5.0  1493850091  heroic bloodshed   \n",
       "100833     610   168250     5.0  1494273047                     \n",
       "100834     610   168252     5.0  1493846352                     \n",
       "100835     610   170875     3.0  1493846415                     \n",
       "\n",
       "                                 title  \\\n",
       "0                     Toy Story (1995)   \n",
       "1              Grumpier Old Men (1995)   \n",
       "2                          Heat (1995)   \n",
       "3          Seven (a.k.a. Se7en) (1995)   \n",
       "4           Usual Suspects, The (1995)   \n",
       "...                                ...   \n",
       "100831                    Split (2017)   \n",
       "100832   John Wick: Chapter Two (2017)   \n",
       "100833                  Get Out (2017)   \n",
       "100834                    Logan (2017)   \n",
       "100835  The Fate of the Furious (2017)   \n",
       "\n",
       "                                             genres  rating_mean  rating_cnt  \\\n",
       "0       adventure|animation|children|comedy|fantasy     3.920930         215   \n",
       "1                                    comedy|romance     3.259615          52   \n",
       "2                             action|crime|thriller     3.946078         102   \n",
       "3                                  mystery|thriller     3.975369         203   \n",
       "4                            crime|mystery|thriller     4.237745         204   \n",
       "...                                             ...          ...         ...   \n",
       "100831                        drama|horror|thriller     3.333333           6   \n",
       "100832                        action|crime|thriller     4.142857           7   \n",
       "100833                                       horror     3.633333          15   \n",
       "100834                                action|sci-fi     4.280000          25   \n",
       "100835                  action|crime|drama|thriller     2.333333           3   \n",
       "\n",
       "        rating_median  rating_variance  rating_mode  \n",
       "0                 4.0         0.693748          4.0  \n",
       "1                 3.0         1.091254          3.0  \n",
       "2                 4.0         0.661308          4.0  \n",
       "3                 4.0         0.846684          4.0  \n",
       "4                 4.5         0.638330          5.0  \n",
       "...               ...              ...          ...  \n",
       "100831            4.0         2.055556          4.0  \n",
       "100832            4.0         0.479592          4.0  \n",
       "100833            4.0         0.882222          3.0  \n",
       "100834            4.5         0.401600          5.0  \n",
       "100835            3.0         0.888889          3.0  \n",
       "\n",
       "[102677 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# соберем данные в одну табличку\n",
    "\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "\n",
    "all_data = ratings.copy()\n",
    "\n",
    "# добавляем к рейтингам теги и приводим их к нижнему регистру\n",
    "all_data = all_data.join(tags[['userId','movieId','tag']].set_index(['userId','movieId']),on=['userId','movieId'],rsuffix='_tags')\n",
    "\n",
    "# добавляем к рейтингам фильмы и приводим их к нижнему регистру\n",
    "all_data = all_data.join(movies.set_index('movieId'),on='movieId',rsuffix='_movies')\n",
    "\n",
    "# добавляем среднюю оценку (mean)\n",
    "ratings_mean = ratings.groupby(['movieId'],as_index=False).agg({'rating':np.mean})\n",
    "all_data = all_data.join(ratings_mean.set_index('movieId'),on='movieId',rsuffix='_mean')\n",
    "\n",
    "# добавляем кол-во отзывов\n",
    "ratings_len = ratings.groupby(['movieId'],as_index=False).agg({'rating':len})\n",
    "all_data = all_data.join(ratings_len.set_index('movieId'),on='movieId',rsuffix='_cnt')\n",
    "\n",
    "# добавляем медианную оценку\n",
    "ratings_median = ratings.groupby(['movieId'],as_index=False).agg({'rating':np.median})\n",
    "all_data = all_data.join(ratings_median.set_index('movieId'),on='movieId',rsuffix='_median')\n",
    "\n",
    "# добавляем оценку вариативности\n",
    "ratings_variance = ratings.groupby(['movieId'],as_index=False).agg({'rating':lambda arr: np.var(arr) if len(arr)>0 else 0.0})\n",
    "all_data = all_data.join(ratings_variance.set_index('movieId'),on='movieId',rsuffix='_variance')\n",
    "\n",
    "# добавляем моду\n",
    "ratings_mode = ratings.groupby(['movieId'],as_index=False).agg({'rating':lambda arr: scipy.stats.mode(arr,keepdims=False)[0]})\n",
    "all_data = all_data.join(ratings_mode.set_index('movieId'),on='movieId',rsuffix='_mode')\n",
    "\n",
    "# приведем все теги и жанры к нижнему регистру, попутно избавившись от NaN\n",
    "all_data['tag'] = all_data['tag'].apply(lambda x: x.lower() if isinstance(x,str) else '')\n",
    "all_data['genres'] = all_data['genres'].apply(lambda x: x.lower() if isinstance(x,str) else '')\n",
    "\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1d2998b-8461-43da-8187-2509b88f567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# преобразуем строковые категориальные признаки с помощью Tf Idf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer, CountVectorizer\n",
    "\n",
    "tfidf_transformer = TfidfVectorizer(tokenizer=lambda x: x.split('|'))\n",
    "X_tag_tfidf = tfidf_transformer.fit_transform(all_data['tag']).toarray()\n",
    "\n",
    "tfidf_transformer = TfidfVectorizer(tokenizer=lambda x: x.split('|'))\n",
    "X_genre_tfidf = tfidf_transformer.fit_transform(all_data['genres']).toarray()\n",
    "\n",
    "# комбинируем tf idf признаки в одно пространство\n",
    "X_tfidf = np.hstack((X_tag_tfidf,X_genre_tfidf))\n",
    "X_tfidf_df = pd.DataFrame(data=X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77a816ef-9ea4-4471-8d3b-21f60fd769ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# скомбинируем данные в более удобное представление\n",
    "\n",
    "DATA = all_data[['userId','title','timestamp','rating_mean','rating_cnt','rating_median','rating_variance','rating_mode','rating']].merge(X_tfidf_df,left_index=True,right_index=True)\n",
    "# change all tfidf column names to string\n",
    "rename_columns = {}\n",
    "for col in DATA.columns:\n",
    "  if type(col) == int or col.isnumeric():\n",
    "    rename_columns[col] = 'tfidf_'+str(col)\n",
    "\n",
    "DATA.rename(columns=rename_columns,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "15899c44-7147-46c7-a2f4-ac214b3073d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "recommender = MyRecommender(models=['random_forest','knn','svd'],model_weight=[0.2,0.3,0.5],meta_opts={'user_id_attr':'userId','item_id_attr':'title'})\n",
    "recommender.fit(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "67bafd01-f63e-455b-99de-be9197fab2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{222: ['Willy Wonka & the Chocolate Factory (1971)',\n",
       "  'Seve (2014)',\n",
       "  'Enter the Void (2009)',\n",
       "  'Jetée, La (1962)',\n",
       "  'Come and See (Idi i smotri) (1985)']}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommender.recommend(user_id=222,num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dccc6657-0e6e-47ad-b3d5-d8e4381753eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{555: [\"Monty Python's Life of Brian (1979)\",\n",
       "  'Belle époque (1992)',\n",
       "  'Crossing Delancey (1988)',\n",
       "  'Autumn Sonata (Höstsonaten) (1978)',\n",
       "  'Raiders of the Lost Ark: The Adaptation (1989)']}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommender.recommend(user_id=555,num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "47efb5ac-4898-47d1-a8c8-e02d1a3dc9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{333: ['NeverEnding Story, The (1984)',\n",
       "  'Bill Hicks: Revelations (1993)',\n",
       "  'Bad Boy Bubby (1993)',\n",
       "  'Belle époque (1992)',\n",
       "  \"Guess Who's Coming to Dinner (1967)\"]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommender.recommend(user_id=333,num=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
